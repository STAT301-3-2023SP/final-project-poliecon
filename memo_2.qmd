---
title: "Second Memo"
author: "Parth Patel & Qin Huang"

format:
  html:
    toc: true
    embed-resources: true
    code-fold: show
    link-external-newwindow: true
    
execute:
  warning: false
  echo: false

from: markdown+emoji 
---

::: {.callout-tip icon=false}

## Github Repo Link

[https://github.com/STAT301-3-2023SP/final-project-poliecon](https://github.com/STAT301-3-2023SP/final-project-poliecon)

:::

## Overview

### Prediction Problem

In this project, we use a survey dataset of American youths to predict whether a specific interviewee voted or not in the 2012 presidential election based on their demographics, experiences with civic education, family and friend influences, and their political values. Therefore, it is a classification problem.

### Modeling Plan

In this memo, we developed a well-working recipe. The comprehensive recipe has six steps: 'step_string2factor' is used to convert character variables to factors; ' step_dummy' is used to convert factor variables into a series of binary (0 and 1) variables; ' step_center' xis used to center variables, which means subtracting the mean of a variable from all its values;  'step_scale' is used to scale variables, which means dividing all the values of a variable by its standard deviation. 

This basic recipe also removes variables with over 20% missing values and uses knn for imputation through 'step_rm' and 'step_impute_knn'. It also involves two more steps: 'step_other' is used to collapse infrequent factor levels into a single "other" level; 'step_nzv' is used to identify and remove predictors that have near-zero variance. 

In our final analysis, we might try to develop a second recipe by using step_impute_bass to replace knn imputation and see whether this new imputation method works better than the original one. 

To evaluate the final model, roc_auc will be used as the main comparison metric.

All codes for this memo can be found within memo_2_code.R and the respective modeling R scripts.

### Current Model Results & Runtimes

For this memo, the one recipe was used with a random forest model and a null model. The random forest model mtry, min_n, and trees hyperparameters were tuned.

The results can be found below.

```{r}
#| warning: false

library(tidyverse)
library(tidymodels)
library(gt)

load("results/rf.rda")
load("results/null.rda")

rf_time <- rf_tictoc %>% 
  filter(model == "Random Forest")

null_time <- null_tictoc %>% 
  filter(model == "Null Model")

time_tib <- purrr::reduce(list(rf_time, null_time), dplyr::full_join) %>% 
  gt() %>% 
  tab_header(title = md("**Model Runtime**"))

time_tib

model_set <- 
  as_workflow_set(
    "rf" = rf_tuned,
    "null" = null_tuned
  )

best_models <- as_tibble(
  rank_results(model_set, rank_metric = "roc_auc", select_best = TRUE) %>% 
  filter(.metric == "roc_auc") %>% 
  select(wflow_id, mean, rank)
)

ranked_mods <- gt(best_models) %>% 
  cols_label(wflow_id = html("Model Type"),
             mean = html("Mean"),
             rank = html("Rank")
  ) %>% 
  tab_header(
    title = md("**Top Ranked Models**"),
    subtitle = "Metric = ROC-AUC"
  )

ranked_mods
```

### Relevant Challenges

The random forest model took half an hour to fit and thus we imagine that it might take longer time to fit an XGBoost model. We will see whether fitting an XGBoost model is manageable. 

Meanwhile, if it is super quick to fit other models that are less computationally expensive, such as regression and knn, we would consider incorporating a few more variables into our models, hoping to build a better-performed model. 